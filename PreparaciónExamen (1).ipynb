{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476a408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/13 09:13:04 WARN Utils: Your hostname, DESKTOP-O781O4L resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth1)\n",
      "22/05/13 09:13:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.1/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/13 09:13:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Examen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7174c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"appl.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef34fa6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (2180340017.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    .option(\"header\", \"true\") \\#Si hay encabezado lo carga (nombres columnas)\u001b[0m\n\u001b[0m                                                                             \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "df = spark.read.csv(\"housing.csv\")\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"housing.csv\")\n",
    "\n",
    "file = spark.read \\\n",
    "    .option(\"header\", \"true\") \\#Si hay encabezado lo carga (nombres columnas)\n",
    "    .option(\"inferSchema\", \"true\") \\#Spark da forma a los datos (si no todo ser√°n strings) \n",
    "    .option(\"nullValue\", \"NA\") \\#Null values se cambia a NA\n",
    "    .option(\"mode\", \"dropMalformed\") \\#Fuera filas malformadas (distintas)\n",
    "    .csv('exercise_files/favourite_animals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5221d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7ceb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44227465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"SparkDFBasics\")\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db22842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"NA\") \\\n",
    "    .option(\"mode\", \"dropMalformed\") \\\n",
    "    .csv('Semana9/airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0097913b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[IATA_CODE: string, AIRPORT: string, CITY: string, STATE: string, COUNTRY: string, LATITUDE: double, LONGITUDE: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdf08676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 7)\n"
     ]
    }
   ],
   "source": [
    "#shape of dataset\n",
    "print((file.count(),len(file.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "182bc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "|IATA_CODE|             AIRPORT|         CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "|      ABE|                null|    Allentown|   PA|    USA|40.65236|  -75.4404|\n",
      "|      ABI|Abilene Regional ...|      Abilene|   TX|    USA|    null|  -99.6819|\n",
      "|      ABQ|Albuquerque Inter...|  Albuquerque|   NM|    USA|35.04022|-106.60919|\n",
      "|      ABR|Aberdeen Regional...|     Aberdeen|   SD|    USA|45.44906| -98.42183|\n",
      "|      ABY|Southwest Georgia...|       Albany|   GA|    USA|31.53552| -84.19447|\n",
      "|      ACK|Nantucket Memoria...|    Nantucket|   MA|    USA|41.25305| -70.06018|\n",
      "|      ACT|Waco Regional Air...|         Waco|   TX|    USA|31.61129| -97.23052|\n",
      "|      ACV|      Arcata Airport|Arcata/Eureka|   CA|    USA|40.97812|-124.10862|\n",
      "|      ACY|Atlantic City Int...|Atlantic City|   NJ|    USA|39.45758| -74.57717|\n",
      "|      ADK|        Adak Airport|         Adak|   AK|    USA|51.87796|-176.64603|\n",
      "|      ADQ|      Kodiak Airport|       Kodiak|   AK|    USA|57.74997|-152.49386|\n",
      "|      AEX|Alexandria Intern...|   Alexandria|   LA|    USA|31.32737| -92.54856|\n",
      "|      AGS|Augusta Regional ...|      Augusta|   GA|    USA|33.36996|  -81.9645|\n",
      "|      AKN| King Salmon Airport|  King Salmon|   AK|    USA| 58.6768|-156.64922|\n",
      "|      ALB|Albany Internatio...|       Albany|   NY|    USA|42.74812| -73.80298|\n",
      "|      ALO|Waterloo Regional...|     Waterloo|   IA|    USA|42.55708| -92.40034|\n",
      "|      AMA|Rick Husband Amar...|     Amarillo|   TX|    USA|35.21937|-101.70593|\n",
      "|      ANC|Ted Stevens Ancho...|    Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|      APN|Alpena County Reg...|       Alpena|   MI|    USA|45.07807| -83.56029|\n",
      "|      ASE|Aspen-Pitkin Coun...|        Aspen|   CO|    USA|39.22316|-106.86885|\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaccb099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file.registerTempTable(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a4a1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=spark.sql(\"\"\"\n",
    "select\n",
    " IATA_CODE,\n",
    " AIRPORT,\n",
    " CITY,\n",
    " STATE,\n",
    " COUNTRY,\n",
    " LATITUDE,\n",
    " LONGITUDE\n",
    "FROM\n",
    " airports\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fdc8718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(IATA_CODE='ABE', AIRPORT=None, CITY='Allentown', STATE='PA', COUNTRY='USA', LATITUDE=40.65236, LONGITUDE=-75.4404),\n",
       " Row(IATA_CODE='ABI', AIRPORT='Abilene Regional Airport', CITY='Abilene', STATE='TX', COUNTRY='USA', LATITUDE=None, LONGITUDE=-99.6819),\n",
       " Row(IATA_CODE='ABQ', AIRPORT='Albuquerque International Sunport', CITY='Albuquerque', STATE='NM', COUNTRY='USA', LATITUDE=35.04022, LONGITUDE=-106.60919),\n",
       " Row(IATA_CODE='ABR', AIRPORT='Aberdeen Regional Airport', CITY='Aberdeen', STATE='SD', COUNTRY='USA', LATITUDE=45.44906, LONGITUDE=-98.42183),\n",
       " Row(IATA_CODE='ABY', AIRPORT='Southwest Georgia Regional Airport', CITY='Albany', STATE='GA', COUNTRY='USA', LATITUDE=31.53552, LONGITUDE=-84.19447)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b820671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/13 09:33:11 WARN SimpleFunctionRegistry: The function null_string replaced a previously registered function.\n",
      "22/05/13 09:33:11 WARN SimpleFunctionRegistry: The function null_num replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def null_string(element):\n",
    "    if (element == None):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return element\n",
    "    \n",
    "spark.udf.register(\"null_string\", lambda x : null_string(x))\n",
    "\n",
    "def null_num(element):\n",
    "    if (element == None):\n",
    "        return 0\n",
    "    else:\n",
    "        return element\n",
    "    \n",
    "spark.udf.register(\"null_num\", lambda x : null_num(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c823800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    IATA_CODE,\n",
    "    null_string(AIRPORT) as AIRPORT,\n",
    "    CITY,\n",
    "    STATE,\n",
    "    COUNTRY,\n",
    "    null_num(LATITUDE) as LATITUDE,\n",
    "    LONGITUDE\n",
    "FROM \n",
    "    airports \n",
    "\"\"\"\n",
    "datos = spark.sql(query).coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"airport_limpio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ec63166",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "|IATA_CODE|             AIRPORT|         CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "|      ABE|                    |    Allentown|   PA|    USA|40.65236|  -75.4404|\n",
      "|      ABI|Abilene Regional ...|      Abilene|   TX|    USA|     0.0|  -99.6819|\n",
      "|      ABQ|Albuquerque Inter...|  Albuquerque|   NM|    USA|35.04022|-106.60919|\n",
      "|      ABR|Aberdeen Regional...|     Aberdeen|   SD|    USA|45.44906| -98.42183|\n",
      "|      ABY|Southwest Georgia...|       Albany|   GA|    USA|31.53552| -84.19447|\n",
      "|      ACK|Nantucket Memoria...|    Nantucket|   MA|    USA|41.25305| -70.06018|\n",
      "|      ACT|Waco Regional Air...|         Waco|   TX|    USA|31.61129| -97.23052|\n",
      "|      ACV|      Arcata Airport|Arcata/Eureka|   CA|    USA|40.97812|-124.10862|\n",
      "|      ACY|Atlantic City Int...|Atlantic City|   NJ|    USA|39.45758| -74.57717|\n",
      "|      ADK|        Adak Airport|         Adak|   AK|    USA|51.87796|-176.64603|\n",
      "|      ADQ|      Kodiak Airport|       Kodiak|   AK|    USA|57.74997|-152.49386|\n",
      "|      AEX|Alexandria Intern...|   Alexandria|   LA|    USA|31.32737| -92.54856|\n",
      "|      AGS|Augusta Regional ...|      Augusta|   GA|    USA|33.36996|  -81.9645|\n",
      "|      AKN| King Salmon Airport|  King Salmon|   AK|    USA| 58.6768|-156.64922|\n",
      "|      ALB|Albany Internatio...|       Albany|   NY|    USA|42.74812| -73.80298|\n",
      "|      ALO|Waterloo Regional...|     Waterloo|   IA|    USA|42.55708| -92.40034|\n",
      "|      AMA|Rick Husband Amar...|     Amarillo|   TX|    USA|35.21937|-101.70593|\n",
      "|      ANC|Ted Stevens Ancho...|    Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|      APN|Alpena County Reg...|       Alpena|   MI|    USA|45.07807| -83.56029|\n",
      "|      ASE|Aspen-Pitkin Coun...|        Aspen|   CO|    USA|39.22316|-106.86885|\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"NA\") \\\n",
    "    .option(\"mode\", \"dropMalformed\") \\\n",
    "    .csv('airport_limpio.csv')\n",
    "\n",
    "file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d650d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32ddd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------+-----------------+--------+-----------------+------------------+\n",
      "|summary| Country|              Age|   Repeat_Visitor|Platform| Web_pages_viewed|            Status|\n",
      "+-------+--------+-----------------+-----------------+--------+-----------------+------------------+\n",
      "|  count|   20000|            20000|            20000|   20000|            20000|             20000|\n",
      "|   mean|    null|         28.53955|           0.5029|    null|           9.5533|               0.5|\n",
      "| stddev|    null|7.888912950773227|0.500004090187782|    null|6.073903499824976|0.5000125004687693|\n",
      "|    min|  Brazil|               17|                0|    Bing|                1|                 0|\n",
      "|    max|Malaysia|              111|                1|   Yahoo|               29|                 1|\n",
      "+-------+--------+-----------------+-----------------+--------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predecir status\n",
    "df=spark.read.csv('Semana12/Log_Reg_dataset.csv',inferSchema=True,header=True)\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8750291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical data to numerical form  (Country y Platform)\n",
    "#import required libraries\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b54bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "search_engine_class = StringIndexer(inputCol=\"Platform\", outputCol=\"Platform_Num\")\n",
    "search_engine_obj = search_engine_class.fit(df)\n",
    "df = search_engine_obj.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd403ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "#one hot encoding\n",
    "search_engine_encoder = OneHotEncoder(inputCol=\"Platform_Num\", outputCol=\"Platform_Vector\")\n",
    "ohe = search_engine_encoder.fit(df)\n",
    "df = ohe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1e6b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_indexer = StringIndexer(inputCol=\"Country\", outputCol=\"Country_Num\").fit(df)\n",
    "df = country_indexer.transform(df)\n",
    "\n",
    "country_encoder = OneHotEncoder(inputCol=\"Country_Num\", outputCol=\"Country_Vector\")\n",
    "ohe = country_encoder.fit(df)\n",
    "df = ohe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d814f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(inputCols=['Platform_Vector','Country_Vector','Age', 'Repeat_Visitor','Web_pages_viewed'], outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d18c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_df=df.select(['features','Status'])\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "319989c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data \n",
    "training_df,test_df=model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff937431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "log_reg_class=LogisticRegression(labelCol='Status')\n",
    "log_reg_model=log_reg_class.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e7011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Predicci√≥n del train\n",
    "train_results=log_reg_model.evaluate(training_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "667a7b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|Status|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|(8,[0,2,5,7],[1.0...|     0|[6.00562598362045...|[0.99754121461469...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[6.00562598362045...|[0.99754121461469...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[6.00562598362045...|[0.99754121461469...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[6.00562598362045...|[0.99754121461469...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.24938125199908...|[0.99477666024094...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.24938125199908...|[0.99477666024094...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.24938125199908...|[0.99477666024094...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.24938125199908...|[0.99477666024094...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[5.24938125199908...|[0.99477666024094...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[4.49313652037771...|[0.98893822635924...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[4.49313652037771...|[0.98893822635924...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[3.73689178875634...|[0.97672651111009...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.98064705713496...|[0.95169212792414...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.98064705713496...|[0.95169212792414...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.98064705713496...|[0.95169212792414...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.98064705713496...|[0.95169212792414...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.98064705713496...|[0.95169212792414...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.22440232551359...|[0.90241954529168...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.22440232551359...|[0.90241954529168...|       0.0|\n",
      "|(8,[0,2,5,7],[1.0...|     0|[2.22440232551359...|[0.90241954529168...|       0.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "176cb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds=train_results.filter(train_results['Status']==1).filter(train_results['prediction']==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd877ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387428266381956"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy on training dataset \n",
    "float(correct_preds)/(training_df.filter(training_df['Status']==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fab4b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set results\n",
    "results=log_reg_model.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01271852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31295876151074337"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results.filter(results['Status']==1).filter(results['prediction']==1).count())/(training_df.filter(training_df['Status']==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82dfb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#confusion matrix\n",
    "true_postives = results[(results.Status == 1) & (results.prediction == 1)].count()\n",
    "true_negatives = results[(results.Status == 0) & (results.prediction == 0)].count()\n",
    "false_positives = results[(results.Status == 0) & (results.prediction == 1)].count()\n",
    "false_negatives = results[(results.Status == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de6f1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345\n",
      "2347\n",
      "146\n",
      "162\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print (true_postives)\n",
    "print (true_negatives)\n",
    "print (false_positives)\n",
    "print (false_negatives)\n",
    "print(true_postives+true_negatives+false_positives+false_negatives)\n",
    "print (results.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fc086bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9353809333865177\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "873c7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9413890004014452\n"
     ]
    }
   ],
   "source": [
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1435b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9384\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "167395ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Unique Key',\n",
       " 'Created Date',\n",
       " 'Closed Date',\n",
       " 'Agency',\n",
       " 'Agency Name',\n",
       " 'Complaint Type',\n",
       " 'Descriptor',\n",
       " 'Location Type',\n",
       " 'Incident Zip',\n",
       " 'Incident Address',\n",
       " 'Street Name',\n",
       " 'Cross Street 1',\n",
       " 'Cross Street 2',\n",
       " 'Intersection Street 1',\n",
       " 'Intersection Street 2',\n",
       " 'Address Type',\n",
       " 'City',\n",
       " 'Landmark',\n",
       " 'Facility Type',\n",
       " 'Status',\n",
       " 'Due Date',\n",
       " 'Resolution Description',\n",
       " 'Resolution Action Updated Date',\n",
       " 'Community Board',\n",
       " 'BBL',\n",
       " 'Borough',\n",
       " 'X Coordinate (State Plane)',\n",
       " 'Y Coordinate (State Plane)',\n",
       " 'Open Data Channel Type',\n",
       " 'Park Facility Name',\n",
       " 'Park Borough',\n",
       " 'Vehicle Type',\n",
       " 'Taxi Company Borough',\n",
       " 'Taxi Pick Up Location',\n",
       " 'Bridge Highway Name',\n",
       " 'Bridge Highway Direction',\n",
       " 'Road Ramp',\n",
       " 'Bridge Highway Segment',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"NA\") \\\n",
    "    .option(\"mode\", \"dropMalformed\") \\\n",
    "    .csv('Examen2/311_Service_Requests_from_2010_to_Present_5perc.csv')\n",
    "\n",
    "file.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
