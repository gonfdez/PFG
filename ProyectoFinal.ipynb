{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final\n",
    "## Procesamiento de Datos\n",
    "##### Gonzalo Fernández Suárez\n",
    "\n",
    "---\n",
    "\n",
    "1) Crear un CALLER para descargar noticias de Cryptomonedas (2.0 puntos de 10)\n",
    "\n",
    "1.1 - Conectarse a la API de twitters para descargar tweets sobre Cryptomonedas (0.5 puntos de 10).\n",
    "\n",
    "1.2 - Conectarse a la API de Gdelt para descargar noticias sobre Cryptomonedas (0.5 puntos de 10).\n",
    "\n",
    "1.3 - Scrapear la web https://coinmarketcal.com/en/news para descargar noticicas sobre Cryptomonedas (1.0 puntos de 10).\n",
    "\n",
    "2) Crear un SERVER de Spark Streamming (1 puntos de 10)\n",
    "\n",
    "3) Hacer un analisis de los mensajes en directo en clase, con ventana de tiempo (1 puntos de 10)\n",
    "\n",
    "4) Aplicar un modelo de analisis de sentimiento (2.5 puntos de 10)\n",
    "\n",
    "4.1 - Aplicar uno de los modelo como Vader Sentiment Analysis, textblob, flair o FinBERT (+0.5 puntos de 10).\n",
    "\n",
    "4.2 - Aplicar todos los modelo: Vader Sentiment Analysis, textblob, flair y FinBERT (+0.5 puntos de 10).\n",
    "\n",
    "4.3 - Crear un modelo de machine learning propio y aplicar dicho modelo a los mensajes (+1.5 puntos de 10).\n",
    "\n",
    "Utilizando los siguientes datos etiquetados manualmente:\n",
    "20220421_FULLDATASET_FOR_UTAD.csv\n",
    "\n",
    "4.3.1 - Utilizar news como texto y final_manual_labelling como variable target para vuestro modelo!\n",
    "\n",
    " \n",
    "\n",
    "5) Guardar los datos del SERVER en SQL marcados por tiempo (1.0 puntos de 10)\n",
    "\n",
    "6) Presentar gráficas de evolución de los sentimiento y comparativa (1.5 puntos de 10)\n",
    "\n",
    "6.1 - Presentar gráficas de evolución de los sentimiento, uno de los modelo como Vader Sentiment Analysis, textblob y flair (+1.0 puntos de 10).\n",
    "\n",
    "6.2 - Presentar gráficas de evolución de los sentimiento de todos los modelo: Vader Sentiment Analysis, textblob y flair (+0.5 puntos de 10).\n",
    "\n",
    "7) Incluir en la comparativa tu propio modelo, y ganas nota máxima si tu modelo es mejeor que los anteriores (+1.0 puntos de 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tutorial instalacion Spark en Windows](https://phoenixnap.com/kb/install-spark-on-windows-10).\n",
    "El link de Java esta mal, descargar JDK no JRE\n",
    "\n",
    "[Runear spark](https://aamargajbhiye.medium.com/apache-spark-setup-a-multi-node-standalone-cluster-on-windows-63d413296971)\n",
    "```\n",
    "cmd >> cd C:\\Users\\User\\Desktop\\U-Tad\\Procesamiento de Datos\\spark-3.2.1-bin-hadoop3.2\n",
    "\n",
    "# Levantar Master Node\n",
    "cmd >> bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n",
    "```\n",
    "http://DESKTOP-GFS:8080\n",
    "\n",
    "```\n",
    "# Poner a escuchar al Worker Node\n",
    "cmd >> bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacion PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Este proyecto está desarrollado enteramente en un sistema **Windows**. Las máquinas virtuales Ubuntu me han dado malas experiencias petandome el ordenador o bien rompiendose dañando toda la informacion que habia en ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scrapear la web https://coinmarketcal.com/en/news para descargar noticicas sobre Cryptomonedas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Crear un SERVER de Spark Streamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el Contexto\n",
    "sc = SparkContext(\"local[2]\" , \"NetworkWordCount\" ) # Master y nombre de la app\n",
    "ssc = StreamingContext(sc, 10) # Contexto de streaming, contexto de spark y cada cuanto se repite el proceso\n",
    "\n",
    "# Inicializamos el socket => Comunicamos el origen de los datos\n",
    "lines = ssc.socketTextStream(\"localhost\", 9090) # Host, Puerto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hacer un analisis de los mensajes en directo, con ventana de tiempo\n",
    "\n",
    "Retocar comentarios del siguiente bloque de codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como vamos a procesar los datos haciendo duplas con las palabras recibidas y el numero de veces que se repiten\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word,1))\n",
    "wordCounts = pairs.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora en la consola de comandos nos conectamos, usando netCat, a \"localhost\" en el puerto 9090. \n",
    "\n",
    "[Instalación netCat en Windows](https://programmerclick.com/article/5414801604/)\n",
    "\n",
    "Abrimos la consola de comandos y ejecutamos:\n",
    "```\n",
    "cmd >> nc -L -p 9090    # Nos conectamos al escuchador del puerto 9090\n",
    "```\n",
    "Ahora desde la cmd podemos enviar información en tiempo real a Spark para que la analice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:20\n",
      "-------------------------------------------\n",
      "('Hola', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:30\n",
      "-------------------------------------------\n",
      "('Soy', 1)\n",
      "('Gonzalo', 1)\n",
      "('probando', 1)\n",
      "('Estoy', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:40\n",
      "-------------------------------------------\n",
      "('como', 1)\n",
      "('Spark', 1)\n",
      "('funciona', 1)\n",
      "('Streaming', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:50\n",
      "-------------------------------------------\n",
      "('mensajes', 1)\n",
      "('en', 1)\n",
      "('Con', 1)\n",
      "('directo', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:24:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:24:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:24:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start() # Iniciamos el Streaming Context\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramos el Streaming Context\n",
    "ssc.stop()\n",
    "# Finalizamos el Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Aplicar un modelo de analisis de sentimiento a los mensajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Aplicar uno de los modelo como Vader Sentiment Analysis, textblob, flair o FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">> pip install vaderSentiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo Server de Spark Streaming para el Vader Sentiment Analysis\n",
    "sc = SparkContext(\"local[2]\" , \"NetworkVaderAnalysis\" ) \n",
    "ssc = StreamingContext(sc, 10) \n",
    "lines = ssc.socketTextStream(\"localhost\", 9090) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de los datos\n",
    "vader = SentimentIntensityAnalyzer() # Inicializamos el anlazidador de Vader\n",
    "# Genero Tuplas con la Linea recibida y su valor 'compound' del Vader Sentiment Analysis\n",
    "linesAnalyzed = lines.map(lambda line: (line,vader.polarity_scores(line)['compound']))\n",
    "# Pinto el resultado\n",
    "linesAnalyzed.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cmd >> nc -L -p 9090    # Nos conectamos al escuchador del puerto 9090\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:50:50\n",
      "-------------------------------------------\n",
      "('hello, good morning', 0.4404)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:00\n",
      "-------------------------------------------\n",
      "('my name is Gonzalo', 0.0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:10\n",
      "-------------------------------------------\n",
      "('im a software lover', 0.5859)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:30\n",
      "-------------------------------------------\n",
      "('and rigth now im testing spark streaming doing a sentiment analysis of my text', 0.2263)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:50\n",
      "-------------------------------------------\n",
      "('i hope this work', 0.4404)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:52:00\n",
      "-------------------------------------------\n",
      "('and i think is working well :)', 0.6249)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:52:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:52:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start() # Iniciamos el Streaming Context\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramos el Streaming Context\n",
    "ssc.stop()\n",
    "# Finalizamos el Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Aplicar todos los modelo: Vader Sentiment Analysis, textblob, flair y FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos al mismo tiempo todos los modelos de Sentiment Analysis a las lineas de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">> pip install vaderSentiment\n",
    ">> pip install textblob\n",
    ">> pip install sentencepiece\n",
    ">> pip install flair   \n",
    ">> pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install -c bioconda flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c bioconda/label/broken flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "!pip install textblob\n",
    "!pip install sentencepiece\n",
    "!pip install flair   \n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "# Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# TextBlob\n",
    "from textblob import TextBlob\n",
    "# Flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "# FinBert\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo Server de Spark Streaming para el Vader Sentiment Analysis\n",
    "# spark = SparkSession.builder.appName('Sentiment Analysis').master('local').getOrCreate()\n",
    "sc = SparkContext(\"local[2]\" , \"NetworkVaderAnalysis\" ) \n",
    "ssc = StreamingContext(sc, 20) \n",
    "lines = ssc.socketTextStream(\"localhost\", 9090) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-05 11:48:40,447 loading file C:\\Users\\gonef\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "# Prerequisitos\n",
    "# Vader\n",
    "vader = SentimentIntensityAnalyzer() # Inicializamos el anlazidador de Vader\n",
    "\n",
    "# Flair\n",
    "classifier = TextClassifier.load('en-sentiment') # Inicializamos el clasificador de Flair en ingles\n",
    "def apply_fair(x):  # Definimos la funcion que aplicará el analisis de Flair a cada texto\n",
    "    sentence = Sentence(x)\n",
    "    classifier.predict(sentence)\n",
    "    return sentence.labels[0].to_dict()['confidence']\n",
    "\n",
    "# FinBert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "def apply_finbert(x):\n",
    "#    return (0,1,2)\n",
    "    inputs = tokenizer([x], padding = True, truncation = True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1) \n",
    "    return predictions[:, 0].tolist()[0], predictions[:, 1].tolist()[0], predictions[:, 2].tolist()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923711180686951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7736659049987793"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(apply_fair('hello, today is a nice day'))\n",
    "apply_finbert('hello, today is a nice day')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como vamos a procesar los datos\n",
    "# Generamos tuplas con la linea analizada y el resultado obtenido de cada tecnica de Sentiment Analysis aplicada\n",
    "vaderPair = lines.map(lambda line: (line,vader.polarity_scores(line)['compound']) ) # Vader\n",
    "textBlobPair = lines.map(lambda line : (line,TextBlob(line).sentiment.polarity) ) # TextBlob\n",
    "# flairPair = lines.map(lambda line: (line, apply_fair(line))) # Flair  Excepcion en .awaitTermination() y no hay forma de solucionarlo\n",
    "finBertPair = lines.map(lambda line: (line, apply_finbert(line)) ) # FinBert  Excepcion en .awaitTermination() y no hay forma de solucionarlo\n",
    "\n",
    "result = vaderPair.join(textBlobPair)#.join(flairPair).join(finBertPair)\n",
    "\n",
    "result.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cmd >> nc -L -p 9090    # Nos conectamos al escuchador del puerto 9090\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4365.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\dstream.py\", line 170, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1568, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\context.py\", line 1227, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (LAPTOP-GFS executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeCommand(PythonRunner.scala:676)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\r\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\r\n\tat com.sun.proxy.$Proxy17.call(Unknown Source)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeCommand(PythonRunner.scala:676)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\r\n\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-2c51d6434685>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Iniciamos el Streaming Context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \"\"\"\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4365.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\dstream.py\", line 170, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1568, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\context.py\", line 1227, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"C:\\Users\\gonef\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (LAPTOP-GFS executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeCommand(PythonRunner.scala:676)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\r\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\r\n\tat com.sun.proxy.$Proxy17.call(Unknown Source)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeCommand(PythonRunner.scala:676)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\r\n\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "ssc.start() # Iniciamos el Streaming Context\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramos el Streaming Context\n",
    "ssc.stop()\n",
    "# Finalizamos el Spark Context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aecd030d4c8316a52bf122072e28f84bcc79844c2684e041fef2e3f1d9f59078"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
