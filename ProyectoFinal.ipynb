{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final\n",
    "## Procesamiento de Datos\n",
    "##### Gonzalo Fernández Suárez\n",
    "\n",
    "---\n",
    "\n",
    "1) Crear un CALLER para descargar noticias de Cryptomonedas (2.0 puntos de 10)\n",
    "\n",
    "1.1 - Conectarse a la API de twitters para descargar tweets sobre Cryptomonedas (0.5 puntos de 10).\n",
    "\n",
    "1.2 - Conectarse a la API de Gdelt para descargar noticias sobre Cryptomonedas (0.5 puntos de 10).\n",
    "\n",
    "1.3 - Scrapear la web https://coinmarketcal.com/en/news para descargar noticicas sobre Cryptomonedas (1.0 puntos de 10).\n",
    "\n",
    "2) Crear un SERVER de Spark Streamming (1 puntos de 10)\n",
    "\n",
    "3) Hacer un analisis de los mensajes en directo en clase, con ventana de tiempo (1 puntos de 10)\n",
    "\n",
    "4) Aplicar un modelo de analisis de sentimiento (2.5 puntos de 10)\n",
    "\n",
    "4.1 - Aplicar uno de los modelo como Vader Sentiment Analysis, textblob, flair o FinBERT (+0.5 puntos de 10).\n",
    "\n",
    "4.2 - Aplicar todos los modelo: Vader Sentiment Analysis, textblob, flair y FinBERT (+0.5 puntos de 10).\n",
    "\n",
    "4.3 - Crear un modelo de machine learning propio y aplicar dicho modelo a los mensajes (+1.5 puntos de 10).\n",
    "\n",
    "Utilizando los siguientes datos etiquetados manualmente:\n",
    "20220421_FULLDATASET_FOR_UTAD.csv\n",
    "\n",
    "4.3.1 - Utilizar news como texto y final_manual_labelling como variable target para vuestro modelo!\n",
    "\n",
    " \n",
    "\n",
    "5) Guardar los datos del SERVER en SQL marcados por tiempo (1.0 puntos de 10)\n",
    "\n",
    "6) Presentar gráficas de evolución de los sentimiento y comparativa (1.5 puntos de 10)\n",
    "\n",
    "6.1 - Presentar gráficas de evolución de los sentimiento, uno de los modelo como Vader Sentiment Analysis, textblob y flair (+1.0 puntos de 10).\n",
    "\n",
    "6.2 - Presentar gráficas de evolución de los sentimiento de todos los modelo: Vader Sentiment Analysis, textblob y flair (+0.5 puntos de 10).\n",
    "\n",
    "7) Incluir en la comparativa tu propio modelo, y ganas nota máxima si tu modelo es mejeor que los anteriores (+1.0 puntos de 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tutorial instalacion Spark en Windows](https://phoenixnap.com/kb/install-spark-on-windows-10).\n",
    "El link de Java esta mal, descargar JDK no JRE\n",
    "\n",
    "[Runear spark](https://aamargajbhiye.medium.com/apache-spark-setup-a-multi-node-standalone-cluster-on-windows-63d413296971)\n",
    "```\n",
    "cmd >> cd C:\\Users\\User\\Desktop\\U-Tad\\Procesamiento de Datos\\spark-3.2.1-bin-hadoop3.2\n",
    "\n",
    "# Levantar Master Node\n",
    "cmd >> bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n",
    "```\n",
    "http://DESKTOP-GFS:8080\n",
    "\n",
    "```\n",
    "# Poner a escuchar al Worker Node\n",
    "cmd >> bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacion PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Este proyecto está desarrollado enteramente en un sistema **Windows**. Las máquinas virtuales Ubuntu me han dado malas experiencias petandome el ordenador o bien rompiendose dañando toda la informacion que habia en ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scrapear la web https://coinmarketcal.com/en/news para descargar noticicas sobre Cryptomonedas (1 PUNTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\gonef\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Para mantener silenciado el webdriver_manager\n",
    "os.environ['WDM_LOG'] = '0'\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.implicitly_wait(10)\n",
    "driver.maximize_window()\n",
    "wait = WebDriverWait(driver,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://coinmarketcal.com/en/news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEX protocol Uniswap hits $1 trillion cumulative trading volume milestone\n",
      "New NFT Marketplace ‘Golom’ Loaded With Analytical Tools Announces Genesis Period\n",
      "Merit Circle DAO proposal seeks to ‘trim the fat’ by booting early-stage investor\n",
      "Tectona’s Bored.Ai Opens Text2Art Minting For The First Time\n",
      "Adam Neumann-backed blockchain startup Flowcarbon raises $70 million\n",
      "Optimism cracks down on airdrop farmers, removing 17,000 addresses\n",
      "Coinbase becomes the first crypto company to enter the Fortune 500\n",
      "HyperEarn Officially Opens Its Token IDO\n",
      "Ted Cruz: ‘I want Texas to be the oasis on planet Earth for bitcoin and crypto’\n",
      "SW DAO Monthly Report: April 2022 Edition\n"
     ]
    }
   ],
   "source": [
    "newsTitles = []\n",
    "titles = driver.find_elements(By.CLASS_NAME, 'card__title')\n",
    "for i in titles:\n",
    "    print (i.text)\n",
    "    newsTitles.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Page\n",
    "nextPageButton = driver.find_element(By.XPATH,'/html/body/main/section[1]/div[2]/div[2]/div/div/nav/ul/li[6]/a')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", nextPageButton)\n",
    "nextPageButton.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['news'] = newsTitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEX protocol Uniswap hits $1 trillion cumulati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New NFT Marketplace ‘Golom’ Loaded With Analyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Merit Circle DAO proposal seeks to ‘trim the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tectona’s Bored.Ai Opens Text2Art Minting For ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adam Neumann-backed blockchain startup Flowcar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Optimism cracks down on airdrop farmers, remov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Coinbase becomes the first crypto company to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HyperEarn Officially Opens Its Token IDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ted Cruz: ‘I want Texas to be the oasis on pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SW DAO Monthly Report: April 2022 Edition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news\n",
       "0  DEX protocol Uniswap hits $1 trillion cumulati...\n",
       "1  New NFT Marketplace ‘Golom’ Loaded With Analyt...\n",
       "2  Merit Circle DAO proposal seeks to ‘trim the f...\n",
       "3  Tectona’s Bored.Ai Opens Text2Art Minting For ...\n",
       "4  Adam Neumann-backed blockchain startup Flowcar...\n",
       "5  Optimism cracks down on airdrop farmers, remov...\n",
       "6  Coinbase becomes the first crypto company to e...\n",
       "7           HyperEarn Officially Opens Its Token IDO\n",
       "8  Ted Cruz: ‘I want Texas to be the oasis on pla...\n",
       "9          SW DAO Monthly Report: April 2022 Edition"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Crear un SERVER de Spark Streamming (1 PUNTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el Contexto\n",
    "sc = SparkContext(\"local[2]\" , \"NetworkWordCount\" ) # Master y nombre de la app\n",
    "ssc = StreamingContext(sc, 10) # Contexto de streaming, contexto de spark y cada cuanto se repite el proceso\n",
    "\n",
    "# Inicializamos el socket => Comunicamos el origen de los datos\n",
    "lines = ssc.socketTextStream(\"localhost\", 9090) # Host, Puerto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hacer un analisis de los mensajes en directo, con ventana de tiempo (1 PUNTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como vamos a procesar los datos haciendo duplas con las palabras recibidas y el numero de veces que se repiten\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word,1))\n",
    "wordCounts = pairs.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora en la consola de comandos nos conectamos, usando netCat, a \"localhost\" en el puerto 9090. \n",
    "\n",
    "[Instalación netCat en Windows](https://programmerclick.com/article/5414801604/)\n",
    "\n",
    "Abrimos la consola de comandos y ejecutamos:\n",
    "```\n",
    "cmd >> nc -L -p 9090    # Nos conectamos al escuchador del puerto 9090\n",
    "```\n",
    "Ahora desde la cmd podemos enviar información en tiempo real a Spark para que la analice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:20\n",
      "-------------------------------------------\n",
      "('Hola', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:30\n",
      "-------------------------------------------\n",
      "('Soy', 1)\n",
      "('Gonzalo', 1)\n",
      "('probando', 1)\n",
      "('Estoy', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:40\n",
      "-------------------------------------------\n",
      "('como', 1)\n",
      "('Spark', 1)\n",
      "('funciona', 1)\n",
      "('Streaming', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:23:50\n",
      "-------------------------------------------\n",
      "('mensajes', 1)\n",
      "('en', 1)\n",
      "('Con', 1)\n",
      "('directo', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:24:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:24:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-01 18:24:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start() # Iniciamos el Streaming Context\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramos el Streaming Context\n",
    "ssc.stop()\n",
    "# Finalizamos el Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Aplicar un modelo de analisis de sentimiento a los mensajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Aplicar uno de los modelo como Vader Sentiment Analysis, textblob, flair o FinBERT (0,5 PUNTOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">> pip install vaderSentiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo Server de Spark Streaming para el Vader Sentiment Analysis\n",
    "sc = SparkContext(\"local[2]\" , \"NetworkVaderAnalysis\" ) \n",
    "ssc = StreamingContext(sc, 10) \n",
    "lines = ssc.socketTextStream(\"localhost\", 9090) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de los datos\n",
    "vader = SentimentIntensityAnalyzer() # Inicializamos el anlazidador de Vader\n",
    "# Genero Tuplas con la Linea recibida y su valor 'compound' del Vader Sentiment Analysis\n",
    "linesAnalyzed = lines.map(lambda line: (line,vader.polarity_scores(line)['compound']))\n",
    "# Pinto el resultado\n",
    "linesAnalyzed.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cmd >> nc -L -p 9090    # Nos conectamos al escuchador del puerto 9090\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:50:50\n",
      "-------------------------------------------\n",
      "('hello, good morning', 0.4404)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:00\n",
      "-------------------------------------------\n",
      "('my name is Gonzalo', 0.0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:10\n",
      "-------------------------------------------\n",
      "('im a software lover', 0.5859)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:30\n",
      "-------------------------------------------\n",
      "('and rigth now im testing spark streaming doing a sentiment analysis of my text', 0.2263)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:51:50\n",
      "-------------------------------------------\n",
      "('i hope this work', 0.4404)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:52:00\n",
      "-------------------------------------------\n",
      "('and i think is working well :)', 0.6249)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:52:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-03 13:52:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start() # Iniciamos el Streaming Context\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramos el Streaming Context\n",
    "ssc.stop()\n",
    "# Finalizamos el Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Aplicar todos los modelo: Vader Sentiment Analysis, textblob, flair y FinBERT (0,5 PUNTOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos al mismo tiempo todos los modelos de Sentiment Analysis a las lineas de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">> pip install vaderSentiment\n",
    ">> pip install textblob\n",
    ">> pip install sentencepiece\n",
    ">> pip install flair   \n",
    ">> pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "# Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# TextBlob\n",
    "from textblob import TextBlob\n",
    "# Flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "# FinBert\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo Server de Spark Streaming para el Vader Sentiment Analysis\n",
    "# spark = SparkSession.builder.appName('Sentiment Analysis').master('local').getOrCreate()\n",
    "sc = SparkContext(\"local[2]\" , \"NetworkVaderAnalysis\" ) \n",
    "ssc = StreamingContext(sc, 10) \n",
    "lines = ssc.socketTextStream(\"localhost\", 9090) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:37:43,756 loading file C:\\Users\\gonef\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "# Prerequisitos\n",
    "# Vader\n",
    "vader = SentimentIntensityAnalyzer() # Inicializamos el anlazidador de Vader\n",
    "\n",
    "# Flair\n",
    "classifier = TextClassifier.load('en-sentiment') # Inicializamos el clasificador de Flair en ingles\n",
    "def apply_fair(x):  # Definimos la funcion que aplicará el analisis de Flair a cada texto\n",
    "    sentence = Sentence(x)\n",
    "    classifier.predict(sentence)\n",
    "    return sentence.labels[0].to_dict()['confidence']\n",
    "\n",
    "# FinBert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "def apply_finbert(x):\n",
    "#    return (0,1,2)\n",
    "    inputs = tokenizer([x], padding = True, truncation = True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1) \n",
    "    return predictions[:, 0].tolist()[0], predictions[:, 1].tolist()[0], predictions[:, 2].tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923711180686951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7736659049987793"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(apply_fair('hello, today is a nice day')) # El analisis con Flair funciona\n",
    "apply_finbert('hello, today is a nice day')[0] # El analisis con FinBert funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como vamos a procesar los datos\n",
    "# Generamos tuplas con la linea analizada y el resultado obtenido de cada tecnica de Sentiment Analysis aplicada\n",
    "vaderPair = lines.map(lambda line: (line, vader.polarity_scores(line)['compound']) ) # Vader\n",
    "textBlobPair = lines.map(lambda line : (line, TextBlob(line).sentiment.polarity) ) # TextBlob\n",
    "#flairPair = lines.map(lambda line: (line, apply_fair(line))) # Flair  Excepcion en .awaitTermination() y no hay forma de solucionarlo\n",
    "#finBertPair = lines.map(lambda line: (line, apply_finbert(line)[0]) ) # FinBert  Excepcion en .awaitTermination() y no hay forma de solucionarlo\n",
    "\n",
    "result = vaderPair.join(textBlobPair)#.join(flairPair).join(finBertPair)\n",
    "\n",
    "result.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cmd >> nc -L -p 9090    # Nos conectamos al escuchador del puerto 9090\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start() # Iniciamos el Streaming Context\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramos el Streaming Context\n",
    "ssc.stop()\n",
    "# Finalizamos el Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Crear un modelo de machine learning propio (1,5 PUNTOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar news como texto y final_manual_labelling como variable target para vuestro modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"ProyectoFinal\")\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Data/20220421_FULLDATASET_FOR_UTAD.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2683, 4)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+----------------------+\n",
      "| id|     date|                news|final_manual_labelling|\n",
      "+---+---------+--------------------+----------------------+\n",
      "|  0|1/25/2022|Ripple announces ...|                     1|\n",
      "|  1|1/25/2022|IMF directors urg...|                    -1|\n",
      "|  2|1/25/2022|Dragonfly Capital...|                     1|\n",
      "|  3|1/25/2022|Rick and Morty co...|                     0|\n",
      "|  4|1/25/2022|How fintech SPACs...|                     0|\n",
      "+---+---------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['final_manual_labelling'] != 0) # Elimino las noticias neutras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn('final_manual_labelling',when(df.final_manual_labelling == -1,0).otherwise(1)) # Cambio los -1 por 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+----------------------+\n",
      "| id|     date|                news|final_manual_labelling|\n",
      "+---+---------+--------------------+----------------------+\n",
      "|  0|1/25/2022|Ripple announces ...|                     1|\n",
      "|  1|1/25/2022|IMF directors urg...|                     0|\n",
      "|  2|1/25/2022|Dragonfly Capital...|                     1|\n",
      "|  5|1/25/2022|Multichain vulner...|                     0|\n",
      "|  8|1/25/2022|GoodDollar Launch...|                     1|\n",
      "|  9|1/25/2022|BCB Group raises ...|                     1|\n",
      "| 10|1/25/2022|Carola Morena to ...|                     1|\n",
      "| 11|1/25/2022|Twitter is growin...|                     1|\n",
      "| 12|1/24/2022|Bitcoin climbs mo...|                     1|\n",
      "| 13|1/24/2022|Walmart director ...|                     1|\n",
      "+---+---------+--------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[id: int, date: string, news: string, final_manual_labelling: int]>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "search_engine_class = StringIndexer(inputCol=\"news\", outputCol=\"news_num\")\n",
    "search_engine_obj = search_engine_class.fit(df)\n",
    "df = search_engine_obj.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+----------------------+--------+\n",
      "| id|     date|                news|final_manual_labelling|news_num|\n",
      "+---+---------+--------------------+----------------------+--------+\n",
      "|  0|1/25/2022|Ripple announces ...|                     1|  1127.0|\n",
      "|  1|1/25/2022|IMF directors urg...|                     0|   789.0|\n",
      "|  2|1/25/2022|Dragonfly Capital...|                     1|   569.0|\n",
      "|  5|1/25/2022|Multichain vulner...|                     0|   955.0|\n",
      "|  8|1/25/2022|GoodDollar Launch...|                     1|   742.0|\n",
      "+---+---------+--------------------+----------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "search_engine_encoder = OneHotEncoder(inputCol=\"news_num\", outputCol=\"news_vector\")\n",
    "ohe = search_engine_encoder.fit(df)\n",
    "df = ohe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+----------------------+--------+-------------------+\n",
      "| id|     date|                news|final_manual_labelling|news_num|        news_vector|\n",
      "+---+---------+--------------------+----------------------+--------+-------------------+\n",
      "|  0|1/25/2022|Ripple announces ...|                     1|  1127.0|(1441,[1127],[1.0])|\n",
      "|  1|1/25/2022|IMF directors urg...|                     0|   789.0| (1441,[789],[1.0])|\n",
      "|  2|1/25/2022|Dragonfly Capital...|                     1|   569.0| (1441,[569],[1.0])|\n",
      "|  5|1/25/2022|Multichain vulner...|                     0|   955.0| (1441,[955],[1.0])|\n",
      "|  8|1/25/2022|GoodDollar Launch...|                     1|   742.0| (1441,[742],[1.0])|\n",
      "+---+---------+--------------------+----------------------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['news_vector'], outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_df=df.select(['features','final_manual_labelling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072, 370)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the data \n",
    "training_df,test_df=model_df.randomSplit([0.75,0.25])\n",
    "(training_df.count(),test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_class=LogisticRegression(labelCol='final_manual_labelling')\n",
    "log_reg_model=log_reg_class.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training Results\n",
    "train_results=log_reg_model.evaluate(training_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability at 0 index is for 0 class and probabilty as 1 index is for 1 class\n",
    "correct_preds=train_results.filter(train_results['final_manual_labelling']==1).filter(train_results['prediction']==1).count()\n",
    "#accuracy on training dataset \n",
    "float(correct_preds)/(training_df.filter(training_df['final_manual_labelling']==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set results\n",
    "results=log_reg_model.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------+\n",
      "|final_manual_labelling|prediction|\n",
      "+----------------------+----------+\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|0                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "|1                     |1.0       |\n",
      "+----------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['final_manual_labelling','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- final_manual_labelling: integer (nullable = false)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#confusion matrix\n",
    "true_postives = results[(results.final_manual_labelling == 1) & (results.prediction == 1)].count()\n",
    "true_negatives = results[(results.final_manual_labelling == 0) & (results.prediction == 0)].count()\n",
    "false_positives = results[(results.final_manual_labelling == 0) & (results.prediction == 1)].count()\n",
    "false_negatives = results[(results.final_manual_labelling == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "0\n",
      "81\n",
      "0\n",
      "370\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "print (true_postives)\n",
    "print (true_negatives)\n",
    "print (false_positives)\n",
    "print (false_negatives)\n",
    "print(true_postives+true_negatives+false_positives+false_negatives)\n",
    "print (results.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7810810810810811\n"
     ]
    }
   ],
   "source": [
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7810810810810811\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Guardar los datos del SERVER en SQL marcados por tiempo (1 PUNTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext \n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"ProyectoFinal\")\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"NA\") \\\n",
    "    .option(\"mode\", \"dropMalformed\") \\\n",
    "    .csv(\"Data/20220421_FULLDATASET_FOR_UTAD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+----------------------+\n",
      "| id|     date|                news|final_manual_labelling|\n",
      "+---+---------+--------------------+----------------------+\n",
      "|  0|1/25/2022|Ripple announces ...|                     1|\n",
      "|  1|1/25/2022|IMF directors urg...|                    -1|\n",
      "|  2|1/25/2022|Dragonfly Capital...|                     1|\n",
      "|  3|1/25/2022|Rick and Morty co...|                     0|\n",
      "|  4|1/25/2022|How fintech SPACs...|                     0|\n",
      "|  5|1/25/2022|Multichain vulner...|                    -1|\n",
      "|  6|1/25/2022|YouTube wants to ...|                     0|\n",
      "|  7|1/25/2022|OpenSea is reimbu...|                     0|\n",
      "|  8|1/25/2022|GoodDollar Launch...|                     1|\n",
      "|  9|1/25/2022|BCB Group raises ...|                     1|\n",
      "+---+---------+--------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-255-1867e0b63f84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregisterTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "#result.registerTemplate(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonef\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"fulldataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c928e0e783906a3e810468759e1381c5a532dd2af0470b73dd01750b6f33afb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
